{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848c4c35-d848-4587-be23-a22f642c4c5a",
   "metadata": {},
   "source": [
    "# Load and clean the data\n",
    "\n",
    "* Load the metadata prepared in `../00_load_metadata.ipynb`\n",
    "* Keep only papers on natural language processing: original category 'cs.CL' (Computation and Language)\n",
    "* Load abstracts prepared in `../00_load_abstracts.ipynb`, merge with metadata dataframe\n",
    "* Check that all entries have an abstract\n",
    "* Keep only research papers (research papers are papers that are not review papers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4dbed2-647d-436d-b08a-6e07b31674f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tok\n",
    "import zipfile as zf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a8c852-a313-4882-a469-316a9c1cf18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 907 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the metadata downloaded from archive\n",
    "arxiv_metadata = pd.read_csv('../data/arxiv_metadata.csv.zip', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf09b7c-c571-46c7-9cd3-8cb5084e4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only papers on natural language processing: original category 'cs.CL' (Computation and Language)\n",
    "nlp_idx = ['cs.CL' in subject for subject in arxiv_metadata['categories']]\n",
    "arxiv_nlp = arxiv_metadata[nlp_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eda3d476-44e7-4222-98c8-805878cf9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 602 ms, total: 11.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load abstracts extracted data in notebook 00_load_abstracts\n",
    "arxiv_abstracts = pd.read_csv('../data/arxiv_abstracts.csv.zip', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1250eba1-791b-49c3-a4b1-1f26c8e65ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with metadata dataframe\n",
    "arxiv_abstracts_nlp = arxiv_abstracts[arxiv_abstracts.id.isin(arxiv_nlp.id)]\n",
    "arxiv_nlp_merged = pd.merge(arxiv_nlp, arxiv_abstracts_nlp, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bafa63b5-b863-4da1-8bb4-43a4e61bd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all entries have an abstract\n",
    "idx = arxiv_nlp_merged['abstract'].isna()\n",
    "arxiv_nlp_merged = arxiv_nlp_merged[~idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "188e72d4-187d-4c7f-83f2-6d2a446ff074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only research papers (research papers are papers that are not review papers).\n",
    "research_paper_idx = pd.Series(['systematic literature review' not in abstract.lower() for abstract in arxiv_nlp_merged.abstract])\n",
    "arxiv_nlp_reviews = arxiv_nlp_merged[~research_paper_idx] \n",
    "arxiv_nlp_merged = arxiv_nlp_merged[research_paper_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78628d4b-547f-4d3a-9064-5c71e7c309a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 review papers on NLP with an abstract in the dataset.\n",
      "There are 54551 research papers on NLP with an abstract in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(arxiv_nlp_reviews)} review papers on NLP with an abstract in the dataset.\")\n",
    "print(f\"There are {len(arxiv_nlp_merged)} research papers on NLP with an abstract in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0d033-2a4b-426d-9ae3-312516f2a951",
   "metadata": {},
   "source": [
    "## Split the data into train / validate / test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ded9e9b8-fc72-477b-b5e0-a5f15ee600f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset has 27275 rows, the validate dataset 13638 rows, the test dataset 13638 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arxiv_nlp_train, arxiv_nlp_test = train_test_split(arxiv_nlp_merged, test_size=0.5)\n",
    "arxiv_nlp_validate, arxiv_nlp_test = train_test_split(arxiv_nlp_test, test_size=0.5)\n",
    "print(f\"The train dataset has {arxiv_nlp_train.shape[0]} rows, the validate dataset {arxiv_nlp_validate.shape[0]} rows, the test dataset {arxiv_nlp_test.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e023b-f356-4f42-afc6-fb59d26537a9",
   "metadata": {},
   "source": [
    "### Save raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55162e94-e1f1-4d16-a6c1-28b99d1c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with zf.ZipFile('../data/arxiv_nlp.csv.zip', 'w') as ziparchive:\n",
    "    ziparchive.writestr('arxiv_nlp.csv', arxiv_nlp_merged.to_csv())\n",
    "\n",
    "with zf.ZipFile('../data/arxiv_nlp_test.csv.zip', 'w') as ziparchive:\n",
    "    ziparchive.writestr('arxiv_nlp_test.csv', arxiv_nlp_test.to_csv())\n",
    "\n",
    "with zf.ZipFile('../data/arxiv_nlp_validate.csv.zip', 'w') as ziparchive:\n",
    "    ziparchive.writestr('arxiv_nlp_validate.csv', arxiv_nlp_validate.to_csv())\n",
    "\n",
    "with zf.ZipFile('../data/arxiv_nlp_train.csv.zip', 'w') as ziparchive:\n",
    "    ziparchive.writestr('arxiv_nlp_train.csv', arxiv_nlp_train.to_csv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a7fb7-9e2c-48e0-8c9a-34ce426416f2",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e81b9a-f917-41a0-9e22-678189217298",
   "metadata": {},
   "source": [
    "Apply pre-processing filters: strip_tags, strip_punctuation, strip_multiple_whitespaces, stric_numeric, remove_stopwords; strip_short\n",
    "\n",
    "Apply lemmatization to the list of words.\n",
    "\n",
    "see: https://github.com/piskvorky/gensim/blob/develop/gensim/parsing/preprocessing.py\n",
    "\n",
    "\"corpus_train\"\n",
    "> A percent of the texts reserved for fitting the model.\n",
    "\n",
    "\"corpus_validate\"\n",
    "> A percent of the texts reserved for computing perplexity when fitting the model's k-parameter, and searching for best parameters.\n",
    "\n",
    "\"corpus_test\"\n",
    "> A percent of the texts reserved for testing hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "def8a678-f5e2-434e-b7ec-b65a10c1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary with all the words in the complete dataset\n",
    "texts = tok.clean(arxiv_nlp_merged['abstract'])\n",
    "dictionary = tok.make_dictionary(texts)\n",
    "\n",
    "# tokenize\n",
    "texts_train = tok.clean(arxiv_nlp_train['abstract'])\n",
    "corpus_train = tok.make_corpus(dictionary, texts_train)\n",
    "\n",
    "texts_validate = tok.clean(arxiv_nlp_validate['abstract'])\n",
    "corpus_validate = tok.make_corpus(dictionary, texts_validate)\n",
    "\n",
    "texts_test = tok.clean(arxiv_nlp_test['abstract'])\n",
    "corpus_test = tok.make_corpus(dictionary, texts_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a149c-83a2-42cd-8292-a62c2472ccf2",
   "metadata": {},
   "source": [
    "### Save tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3603c7d-83b7-4405-923f-1695f051fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/dictionary.pickle', 'wb') as handle:\n",
    "    pickle.dump(dictionary, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../data/corpus_train.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpus_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../data/corpus_validate.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpus_validate, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../data/corpus_test.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpus_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75e678-5fe1-4b73-8153-4332339dfad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

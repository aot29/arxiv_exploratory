{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed50f3b9-68b5-48cb-8943-5948ed00a7ba",
   "metadata": {},
   "source": [
    "# Recurse\n",
    "\n",
    "1. Load the trained LDA model\n",
    "2. Load the train data.\n",
    "3. Assign topics to documents in the train dataset\n",
    "4. For each set of documents, train a LDA model\n",
    "5. save the trained models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f228f1ac-78a7-4212-84c2-60a2301212c1",
   "metadata": {},
   "source": [
    "### Load the trained model\n",
    "The model was trained in notebook 01_Train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4ebf6f-b649-447f-9901-da90cb2be025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#ldaModel = LdaModel.load('../models/lda_nlp2.model')\n",
    "with open('../models/lda_nlp_train.pickle', 'rb') as handle:\n",
    "    ldamodel = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da2d63-8253-44a4-8001-4bc5807e8224",
   "metadata": {},
   "source": [
    "## Load the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11970751-2b3f-4347-bf6b-5f4d4c9b9562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85123b3c-5a2a-4eb6-b7f2-b749106af118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 281 ms, sys: 44.1 ms, total: 325 ms\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arxiv_nlp_train = pd.read_csv('../data/arxiv_nlp_train.csv.zip', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb75d836-9778-4ec3-acc8-b5d460c0f58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27275 in the train dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(arxiv_nlp_train)} in the train dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac230ffe-2151-4102-a310-c4c89e939097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tok\n",
    "\n",
    "texts = tok.clean(arxiv_nlp_train['abstract'])\n",
    "dictionary, corpus = tok.dict_corpus(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48146927-7df9-40e6-8f28-955f6aa55d5c",
   "metadata": {},
   "source": [
    "## Assign topics to the data\n",
    "\n",
    "Aggregate topic information in a dataframe (see: https://campus.datacamp.com/courses/fraud-detection-in-python/fraud-detection-using-text?ex=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76f1732-ca74-432a-9db9-2215117cf81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_details(ldamodel, corpus):\n",
    "    topic_details_list = []\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_details_list.append([topic_num, prop_topic, row])\n",
    "    topic_details_df = pd.DataFrame(topic_details_list)\n",
    "    topic_details_df.columns = ['Dominant_Topic', '% Score', 'Topics']\n",
    "    return topic_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68159dd-b4c9-42a2-b5de-9bfe70f5259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_topics(ldamodel, corpus, df):\n",
    "    # put the arxiv id, original categories data and creation date in a dataframe\n",
    "    # combine with result of topic details function\n",
    "    topics_df = pd.DataFrame()\n",
    "    topic_details = get_topic_details(ldamodel, corpus)\n",
    "    topics_df['id'] = list(df['id'])\n",
    "    topics_df['year'] = list(df['year'])\n",
    "    topics_df['month'] = list(df['month'])\n",
    "    topics_df['Dominant Topic'] = topic_details['Dominant_Topic']\n",
    "    topics_df['% Score'] = topic_details['% Score']\n",
    "    topics_df['Topics'] = topic_details['Topics']\n",
    "    return topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c083a570-4e37-47f9-a1c6-4f3c0ccb6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_nlp_train = assign_topics(ldamodel, corpus, arxiv_nlp_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde73fb1-da92-4c8d-b47d-7455b4d5d181",
   "metadata": {},
   "source": [
    "## For each set of documents, train a LDA model\n",
    "Fit the model on the training set, for different values of k.\n",
    "\n",
    "Validate the model by computing perplexity for different values of k. The best value for k is that which yields a perplexity closest to 0.\n",
    "\n",
    "Since LDA has a random component, the best k might vary between runs. Therefore the model is fitted several times and the best value is compiled from all runs (mode of best k for all runs).\n",
    "\n",
    "see: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67e84b2d-8139-459c-be93-b12c9cf7714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_runs = 5\n",
    "min_k = 2\n",
    "max_k = 12\n",
    "\n",
    "def fit_lda(dictionary, corpus):\n",
    "    perplexity = []\n",
    "    for run in range(max_runs):\n",
    "        print(f\"Run {run + 1} / {max_runs}\")\n",
    "        px = []\n",
    "        num_topics = []\n",
    "        for k in range(min_k, max_k + min_k):\n",
    "            # Define the LDA model\n",
    "            ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=k, id2word=dictionary, passes=15)\n",
    "            num_topics.append(k)\n",
    "            px.append(ldamodel.log_perplexity(corpus))\n",
    "        perplexity.append(pd.DataFrame.from_dict({'k': num_topics, 'val': px}))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df93ee05-601f-4e49-865a-db58dbaf371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_k_from_perplexity(perplexity):\n",
    "    best_k_runs = []\n",
    "    for run in range(max_runs):\n",
    "        best_k_runs.append(perplexity[run].sort_values('val', ascending=False).iloc[0]['k'])\n",
    "    best_k = statistics.mode(best_k_runs)\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04ff020-5fcc-443a-b965-2f7152087f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "Run 1 / 5\n",
      "Run 2 / 5\n",
      "Run 3 / 5\n",
      "Run 4 / 5\n",
      "Run 5 / 5\n",
      "CPU times: user 1h 44min 56s, sys: 420 ms, total: 1h 44min 56s\n",
      "Wall time: 1h 44min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "topics = set(topics_nlp_train['Dominant Topic'])\n",
    "topic_model = {}\n",
    "for topic in topics:\n",
    "    idx = topics_nlp_train['Dominant Topic'] == topic\n",
    "    if (idx.sum() == 0):\n",
    "        topic_model[topic] = None\n",
    "        continue\n",
    "    df_topic = arxiv_nlp_train.reset_index()[idx]\n",
    "    texts = tok.clean(df_topic['abstract'])\n",
    "    dictionary, corpus = tok.dict_corpus(texts)\n",
    "    if not any(corpus):\n",
    "        topic_model[topic] = None\n",
    "        continue\n",
    "    perplexity = fit_lda(dictionary, corpus)\n",
    "    best_k = best_k_from_perplexity(perplexity)\n",
    "    topic_model[topic] = gensim.models.ldamodel.LdaModel(corpus, num_topics=best_k, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00b39d40-3da2-4537-86bf-e505e78c04a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "(0, '0.053*\"word\" + 0.032*\"embed\" + 0.018*\"represent\" + 0.017*\"task\" + 0.016*\"method\"')\n",
      "(1, '0.016*\"semant\" + 0.011*\"structur\" + 0.010*\"process\" + 0.010*\"syntact\" + 0.010*\"natur\"')\n",
      "(2, '0.013*\"concept\" + 0.011*\"topic\" + 0.011*\"semant\" + 0.010*\"embed\" + 0.010*\"similar\"')\n",
      "(3, '0.024*\"word\" + 0.015*\"text\" + 0.010*\"base\" + 0.009*\"analysi\" + 0.008*\"us\"')\n",
      "(4, '0.016*\"gener\" + 0.015*\"base\" + 0.012*\"pars\" + 0.012*\"algorithm\" + 0.009*\"problem\"')\n",
      "\n",
      "Topic 1\n",
      "(0, '0.043*\"llm\" + 0.015*\"larg\" + 0.015*\"prompt\" + 0.013*\"gpt\" + 0.011*\"reason\"')\n",
      "(1, '0.017*\"code\" + 0.015*\"learn\" + 0.014*\"problem\" + 0.012*\"natur\" + 0.012*\"program\"')\n",
      "(2, '0.020*\"explan\" + 0.013*\"human\" + 0.012*\"predict\" + 0.012*\"method\" + 0.010*\"learn\"')\n",
      "(3, '0.051*\"text\" + 0.020*\"video\" + 0.012*\"sentenc\" + 0.011*\"stori\" + 0.010*\"base\"')\n",
      "(4, '0.042*\"evalu\" + 0.024*\"metric\" + 0.023*\"human\" + 0.021*\"summar\" + 0.017*\"summari\"')\n",
      "(5, '0.046*\"imag\" + 0.031*\"visual\" + 0.019*\"caption\" + 0.018*\"modal\" + 0.015*\"train\"')\n",
      "(6, '0.067*\"question\" + 0.056*\"answer\" + 0.020*\"knowledg\" + 0.017*\"retriev\" + 0.015*\"reason\"')\n",
      "\n",
      "Topic 2\n",
      "(0, '0.034*\"cross\" + 0.033*\"entiti\" + 0.029*\"lingual\" + 0.022*\"word\" + 0.020*\"relat\"')\n",
      "(1, '0.046*\"resourc\" + 0.039*\"translat\" + 0.033*\"multilingu\" + 0.029*\"low\" + 0.020*\"speech\"')\n",
      "(2, '0.037*\"text\" + 0.019*\"style\" + 0.019*\"transfer\" + 0.013*\"represent\" + 0.012*\"dataset\"')\n",
      "(3, '0.053*\"adversari\" + 0.040*\"attack\" + 0.023*\"exampl\" + 0.022*\"annot\" + 0.021*\"robust\"')\n",
      "(4, '0.028*\"sentenc\" + 0.026*\"label\" + 0.022*\"gener\" + 0.019*\"supervis\" + 0.014*\"approach\"')\n",
      "(5, '0.017*\"gener\" + 0.014*\"distribut\" + 0.012*\"predict\" + 0.011*\"sampl\" + 0.011*\"label\"')\n",
      "(6, '0.038*\"shot\" + 0.023*\"dataset\" + 0.022*\"zero\" + 0.013*\"set\" + 0.012*\"question\"')\n",
      "(7, '0.032*\"label\" + 0.022*\"classif\" + 0.019*\"supervis\" + 0.019*\"text\" + 0.016*\"sampl\"')\n",
      "(8, '0.088*\"domain\" + 0.019*\"adapt\" + 0.017*\"detect\" + 0.017*\"target\" + 0.015*\"intent\"')\n",
      "\n",
      "Topic 3\n",
      "(0, '0.060*\"entiti\" + 0.014*\"inform\" + 0.014*\"relat\" + 0.013*\"type\" + 0.012*\"name\"')\n",
      "(1, '0.061*\"knowledg\" + 0.023*\"graph\" + 0.017*\"dialogu\" + 0.016*\"method\" + 0.013*\"state\"')\n",
      "(2, '0.037*\"graph\" + 0.028*\"text\" + 0.020*\"inform\" + 0.018*\"structur\" + 0.015*\"languag\"')\n",
      "(3, '0.044*\"document\" + 0.023*\"queri\" + 0.020*\"summar\" + 0.019*\"extract\" + 0.016*\"sentenc\"')\n",
      "(4, '0.024*\"clinic\" + 0.018*\"medic\" + 0.017*\"data\" + 0.014*\"languag\" + 0.012*\"patient\"')\n",
      "(5, '0.021*\"label\" + 0.018*\"text\" + 0.016*\"classif\" + 0.015*\"sequenc\" + 0.014*\"bert\"')\n",
      "(6, '0.055*\"event\" + 0.018*\"aspect\" + 0.016*\"extract\" + 0.016*\"tempor\" + 0.014*\"sentiment\"')\n",
      "(7, '0.075*\"relat\" + 0.042*\"extract\" + 0.024*\"report\" + 0.020*\"causal\" + 0.015*\"gener\"')\n",
      "(8, '0.041*\"code\" + 0.030*\"corefer\" + 0.025*\"match\" + 0.022*\"sql\" + 0.020*\"resolut\"')\n",
      "(9, '0.030*\"network\" + 0.025*\"neural\" + 0.023*\"attent\" + 0.018*\"featur\" + 0.017*\"question\"')\n",
      "\n",
      "Topic 4\n",
      "(0, '0.091*\"speaker\" + 0.026*\"voic\" + 0.014*\"audio\" + 0.013*\"method\" + 0.013*\"convers\"')\n",
      "(1, '0.035*\"decod\" + 0.028*\"sequenc\" + 0.016*\"encod\" + 0.013*\"base\" + 0.013*\"attent\"')\n",
      "(2, '0.039*\"gener\" + 0.021*\"text\" + 0.021*\"tt\" + 0.016*\"synthesi\" + 0.014*\"qualiti\"')\n",
      "(3, '0.023*\"data\" + 0.020*\"recognit\" + 0.017*\"represent\" + 0.017*\"learn\" + 0.017*\"text\"')\n",
      "(4, '0.077*\"translat\" + 0.026*\"machin\" + 0.018*\"english\" + 0.017*\"system\" + 0.016*\"evalu\"')\n",
      "(5, '0.034*\"word\" + 0.033*\"charact\" + 0.027*\"segment\" + 0.027*\"level\" + 0.021*\"task\"')\n",
      "(6, '0.039*\"asr\" + 0.025*\"error\" + 0.015*\"recognit\" + 0.014*\"automat\" + 0.013*\"word\"')\n",
      "(7, '0.079*\"languag\" + 0.020*\"word\" + 0.017*\"resourc\" + 0.013*\"english\" + 0.012*\"multilingu\"')\n",
      "(8, '0.067*\"translat\" + 0.024*\"nmt\" + 0.022*\"machin\" + 0.018*\"sentenc\" + 0.016*\"languag\"')\n",
      "(9, '0.041*\"end\" + 0.020*\"neural\" + 0.020*\"network\" + 0.015*\"recognit\" + 0.014*\"base\"')\n",
      "(10, '0.075*\"network\" + 0.042*\"neural\" + 0.032*\"layer\" + 0.020*\"deep\" + 0.019*\"word\"')\n",
      "\n",
      "Topic 5\n",
      "(0, '0.017*\"distil\" + 0.014*\"larg\" + 0.014*\"comput\" + 0.014*\"effici\" + 0.012*\"paramet\"')\n",
      "(1, '0.028*\"tune\" + 0.020*\"fine\" + 0.017*\"data\" + 0.016*\"prompt\" + 0.014*\"learn\"')\n",
      "(2, '0.034*\"attent\" + 0.030*\"transform\" + 0.023*\"layer\" + 0.018*\"token\" + 0.016*\"sequenc\"')\n",
      "(3, '0.023*\"pre\" + 0.018*\"bert\" + 0.016*\"learn\" + 0.013*\"base\" + 0.013*\"represent\"')\n",
      "(4, '0.011*\"transform\" + 0.011*\"network\" + 0.011*\"learn\" + 0.011*\"neural\" + 0.010*\"method\"')\n",
      "\n",
      "Topic 6\n",
      "(0, '0.047*\"llm\" + 0.019*\"polit\" + 0.016*\"larg\" + 0.014*\"prompt\" + 0.013*\"social\"')\n",
      "(1, '0.038*\"dialogu\" + 0.023*\"task\" + 0.023*\"agent\" + 0.018*\"learn\" + 0.016*\"dialog\"')\n",
      "(2, '0.027*\"data\" + 0.012*\"translat\" + 0.010*\"process\" + 0.008*\"us\" + 0.008*\"student\"')\n",
      "(3, '0.027*\"research\" + 0.014*\"process\" + 0.012*\"paper\" + 0.011*\"develop\" + 0.011*\"nlp\"')\n",
      "(4, '0.071*\"convers\" + 0.021*\"topic\" + 0.020*\"respons\" + 0.019*\"gener\" + 0.017*\"dialogu\"')\n",
      "(5, '0.023*\"gener\" + 0.021*\"respons\" + 0.018*\"user\" + 0.015*\"persona\" + 0.011*\"person\"')\n",
      "(6, '0.044*\"commun\" + 0.014*\"network\" + 0.014*\"social\" + 0.014*\"data\" + 0.013*\"us\"')\n",
      "(7, '0.031*\"user\" + 0.024*\"learn\" + 0.022*\"human\" + 0.021*\"interact\" + 0.012*\"robot\"')\n",
      "(8, '0.046*\"speech\" + 0.033*\"messag\" + 0.031*\"sentenc\" + 0.023*\"argument\" + 0.021*\"write\"')\n",
      "(9, '0.023*\"health\" + 0.022*\"social\" + 0.017*\"mental\" + 0.013*\"media\" + 0.011*\"data\"')\n",
      "(10, '0.033*\"emot\" + 0.013*\"review\" + 0.011*\"user\" + 0.011*\"text\" + 0.010*\"featur\"')\n",
      "(11, '0.028*\"user\" + 0.024*\"question\" + 0.020*\"answer\" + 0.020*\"custom\" + 0.019*\"product\"')\n",
      "(12, '0.036*\"bia\" + 0.025*\"gender\" + 0.023*\"bias\" + 0.010*\"group\" + 0.010*\"text\"')\n",
      "\n",
      "Topic 7\n",
      "(0, '0.014*\"claim\" + 0.013*\"legal\" + 0.012*\"fact\" + 0.010*\"check\" + 0.010*\"task\"')\n",
      "(1, '0.020*\"corpu\" + 0.017*\"english\" + 0.016*\"resourc\" + 0.015*\"text\" + 0.013*\"translat\"')\n",
      "(2, '0.024*\"hate\" + 0.024*\"detect\" + 0.022*\"speech\" + 0.016*\"dataset\" + 0.012*\"content\"')\n",
      "(3, '0.022*\"annot\" + 0.013*\"dialect\" + 0.012*\"analysi\" + 0.012*\"data\" + 0.010*\"arab\"')\n",
      "(4, '0.058*\"task\" + 0.018*\"score\" + 0.016*\"perform\" + 0.016*\"train\" + 0.015*\"classif\"')\n",
      "(5, '0.029*\"sentiment\" + 0.019*\"social\" + 0.017*\"tweet\" + 0.016*\"media\" + 0.015*\"analysi\"')\n",
      "(6, '0.047*\"new\" + 0.018*\"articl\" + 0.014*\"fake\" + 0.013*\"topic\" + 0.013*\"inform\"')\n",
      "(7, '0.024*\"dataset\" + 0.019*\"research\" + 0.015*\"data\" + 0.013*\"nlp\" + 0.013*\"entiti\"')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in topics:\n",
    "    print(f\"Topic {topic}\")\n",
    "    if not topic_model[topic]:\n",
    "        print(\"Empty topic list\\n\")\n",
    "        continue\n",
    "    model_topics = topic_model[topic].print_topics(num_words=5)\n",
    "    for model_topic in model_topics:\n",
    "        print(model_topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf6ecbf-342d-4293-9964-a63dab4c903e",
   "metadata": {},
   "source": [
    "## Save the models\n",
    "save a dictionary of models, one for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34d62f2e-c640-4b4e-b816-be91cb9e0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../models/topic_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(topic_model, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554c854-caa2-4026-8bbb-0e008d5f7281",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

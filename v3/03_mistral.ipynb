{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32a6019-86d2-4516-9750-2ded9339c248",
   "metadata": {},
   "source": [
    "# Label and describe using an LLM\n",
    "\n",
    "Download and instantiate an LLM from Huggingface.\n",
    "\n",
    "Load the LDA topic models. \n",
    "\n",
    "Prompt the LLM to generate a label and a description for each topic in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624a770b-cedd-4e31-887b-acc692a3b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/arxiv_exp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/atroncos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253401c-3595-40eb-9968-9a5d6628a8b9",
   "metadata": {},
   "source": [
    "Load the topic models fitted in a previous notebook.\n",
    "\n",
    "* lda_gw: Gravitational Waves topics\n",
    "* lda_cscl: Computation and Language topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542421d1-a7ff-4cf6-a4e1-9b9cff4ecf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/lda_gw.pickle', 'rb') as handle:\n",
    "    lda_gw = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64b0ca-dbb5-41c0-9381-3a163f4341e6",
   "metadata": {},
   "source": [
    "Get a list of all topics in the model, each topic described by MAX_WORDS \n",
    "\n",
    "* The result is a list of topics. Each topic is represented by a tuple.\n",
    "* The first element of the tuple is a topic number (int).\n",
    "* The second element of the tuple is a list of tuples,\n",
    "* Each tuple represents the words characterising he topic (string) and its corresponding probability (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85b7775-3cd3-4610-982c-4f1ffa5be5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30\n",
    "# list[tuples<int, list[tuple<string, float>]>]\n",
    "topics_gw = lda_gw.show_topics(num_words=MAX_WORDS, formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbcd20-4cb4-491c-bfe0-5eb1ca4ad227",
   "metadata": {},
   "source": [
    "### Using gated models\n",
    "\n",
    "1. Go to huggingface, login, go to `settings/access tokens` \n",
    "2. Create a new READ token, save it to ../token.txt\n",
    "3. Go here: https://huggingface.co/mistralai/Mistral-7B-v0.1 and accept the usage conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a255ab19-9ca0-46e8-ac85-127f1eddec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/atroncos/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "with open('../token.txt', 'r') as handle:\n",
    "    token = handle.read()\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95683a43-5540-4701-9dd2-b2ea7650dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_str(topic):\n",
    "    \"\"\"Return the terms describing a topic as a string\n",
    "    topic: list of tuples<string, float>\n",
    "    \"\"\"\n",
    "    terms = [term[0] for term in topic[1]]\n",
    "    resp = ', '.join([term[0] for term in topic[1]])\n",
    "    return(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7c22d7f-ab42-4f45-a7b6-c23fa717664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import disk_offload\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Promptable(ABC):\n",
    "    @abstractmethod\n",
    "    def one_shot(self, prompt): pass\n",
    "\n",
    "class Mistral(Promptable):\n",
    "    def __init__(self):\n",
    "        self.model_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16, low_cpu_mem_usage = True).cpu()\n",
    "        disk_offload(model=self.model, offload_dir=\"alpha\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "    \n",
    "    def one_shot(self, prompt):\n",
    "        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(\"cpu\")\n",
    "        generated_ids = self.model.generate(**model_inputs, max_new_tokens=10, do_sample=True)\n",
    "        decoded_outputs = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        print(f\"decoded_outputs: {decoded_outputs}\")\n",
    "        resp = nltk.sent_tokenize(decoded_outputs.strip())[2]  # split into sentences\n",
    "        print(f\"resp: {resp}\")\n",
    "        return(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "525b21e7-be0e-4ffe-b2b9-ef437ea33d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic_labels(topics, model):\n",
    "    \"\"\"\n",
    "    Predict label for a list of topics.\n",
    "    returns: dataFrame with columns: topic id, label\n",
    "    \"\"\"\n",
    "    topics_range = [topic[0] for topic in topics]\n",
    "    labels = []\n",
    "    resp = []\n",
    "    for topic_id in topics_range:\n",
    "        print(f\"Processing topic {topic_id} / {len(topics_range)}\")\n",
    "        terms = get_topic_str(topics[topic_id])\n",
    "        prompt = f\"What is the label for the physics of gravitational waves topic described by these terms: {terms}? Output only the label.\"\n",
    "        label = model.one_shot(prompt)\n",
    "        labels.append(label)\n",
    "        print(f\"label: {label}\")\n",
    "        break\n",
    "    print(topics_range)\n",
    "    print(labels)\n",
    "    \n",
    "    # this fails because break in loop\n",
    "#    return(pd.DataFrame.from_dict({'topic': topics_range, 'label': labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fc64b9a-f9dd-4c94-bd92-811f2f0c96bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.82s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 0 / 4\n",
      "decoded_outputs: What is the label for the the physics of gravitational waves topic described by these terms: detector, signal, data, noise, frequency, search, time, method, detection, pulsar, sensitivity, based, source, ligo, interferometer, parameter, analysis, timing, model, space, new, test, present, sky, result, measurement, limit, laser, lisa, mode? Output only the label. gravitational_waves: detector, signal,\n",
      "resp: gravitational_waves: detector, signal,\n",
      "label: gravitational_waves: detector, signal,\n",
      "[0, 1, 2, 3]\n",
      "['gravitational_waves: detector, signal,']\n",
      "CPU times: user 9min 51s, sys: 1min 3s, total: 10min 55s\n",
      "Wall time: 7min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mistral = Mistral()\n",
    "predict_topic_labels(topics_gw, mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f7278-6ffc-4ddb-9185-d30f833b02bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

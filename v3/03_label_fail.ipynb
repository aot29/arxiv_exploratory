{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7ab28c-809c-4d44-99f9-c7b0d07a05e1",
   "metadata": {},
   "source": [
    "# Label and describe using an LLM\n",
    "\n",
    "Download and instantiate an LLM from Huggingface.\n",
    "\n",
    "Load the LDA topic models. \n",
    "\n",
    "Prompt the LLM to generate a label and a description for each topic in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98109988-61ca-45e4-ab5c-fda635f632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8b476-7c5d-418b-8e51-211317544f94",
   "metadata": {},
   "source": [
    "Load the topic models fitted in a previous notebook.\n",
    "\n",
    "* lda_gw: Gravitational Waves topics\n",
    "* lda_cscl: Computation and Language topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fdf5aa-e710-4ae9-8494-e847cedad5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/lda_gw.pickle', 'rb') as handle:\n",
    "    lda_gw = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74281fb0-6242-4641-9731-e938655833b2",
   "metadata": {},
   "source": [
    "Get a list of all topics in the model, each topic described by MAX_WORDS \n",
    "\n",
    "* The result is a list of topics. Each topic is represented by a tuple.\n",
    "* The first element of the tuple is a topic number (int).\n",
    "* The second element of the tuple is a list of tuples,\n",
    "* Each tuple represents the words characterising he topic (string) and its corresponding probability (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3b4fbba-e19b-4fef-a032-2279fa47da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30\n",
    "# list[tuples<int, list[tuple<string, float>]>]\n",
    "topics_gw = lda_gw.show_topics(num_words=MAX_WORDS, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "49b83873-be16-4ffc-99fa-5e812fa1fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'field equation star mode theory neutron general state gravity solution energy effect magnetic matter quantum time result relativity non einstein frequency metric study spacetime radiation eos instability relativistic electromagnetic rotating'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_str = ' '.join([topic[0] for topic in topics_gw[0][1]])\n",
    "topic_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf11c3b-2fa3-4e16-ae50-f86d1dfc072d",
   "metadata": {},
   "source": [
    "## Setup a LLM pipeline for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2680bc96-ca0f-4394-b8a6-f7ab05e69ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.11313198506832123, 'token': 4973, 'token_str': 'examples', 'sequence': 'examples : field, equation, star, mode, theory'}, {'score': 0.03067971207201481, 'token': 5097, 'token_str': 'applications', 'sequence': 'applications : field, equation, star, mode, theory'}, {'score': 0.02635965868830681, 'token': 3602, 'token_str': 'note', 'sequence': 'note : field, equation, star, mode, theory'}, {'score': 0.024120096117258072, 'token': 4696, 'token_str': 'category', 'sequence': 'category : field, equation, star, mode, theory'}, {'score': 0.019342072308063507, 'token': 2742, 'token_str': 'example', 'sequence': 'example : field, equation, star, mode, theory'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "labeller = pipeline('fill-mask', model=model_name)\n",
    "outputs = labeller(f\"[MASK]: {topic_str}\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8cce883-3742-49d8-b7e1-4a42a200b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Label this sequence of words field, equation, star, mode, theory is is so so'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google-bert/bert-base-uncased'\n",
    "labeller = pipeline('text-generation', model=model_name)\n",
    "prompt = f\"Label this sequence of words {topic_str} is\"\n",
    "outputs = labeller(prompt, max_new_tokens=3)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0388aef-ffdd-482a-af95-c7d696097ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your min_length=30 must be inferior than your max_length=3.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'field,'}]\n"
     ]
    }
   ],
   "source": [
    "#model_name = 'google-bert/bert-base-uncased'\n",
    "#model_name = 'cnicu/t5-small-booksum'\n",
    "model_name = 't5-small'\n",
    "labeller = pipeline('summarization', model=model_name)\n",
    "outputs = labeller(topic_str, max_length=3)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a40dc9f1-77e2-4ff6-ba1e-c2fc66201f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Which topic is described by these keywords (response should be between 1 and 12 words): field equation star mode theory neutron general state gravity solution energy effect magnetic matter quantum time result relativity non einstein frequency metric study spacetime radiation eos instability relativistic electromagnetic rotating motion motion tectonic acceleration zero gravity system acceleration'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "labeller = pipeline(\"text-generation\", model=model_name)\n",
    "prompt = f\"Which topic is described by these keywords (response should be between 1 and 12 words): {topic_str}\"\n",
    "outputs = labeller(prompt, max_new_tokens=10, pad_token_id=labeller.tokenizer.eos_token_id)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "acdf9a3e-5949-4de4-aa0a-48e581afe6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text\n",
      " field, equation, star, mode, theory\n",
      "\n",
      "Summary\n",
      " field, equation, star, mode, theory\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer.encode(\"summarize: \" + topic_str, return_tensors='pt', max_length=512, truncation=True)\n",
    "summary_ids = model.generate(input_ids, max_length=10)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal text\\n\", topic_str)\n",
    "print(\"\\nSummary\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c801d-446c-4167-af26-52165f889f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", device='cpu', model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b75ea201-85f0-487c-98ac-6e6bdab0946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/atroncos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field equation star mode theory neutron general state gravity solution\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#model_name = 'fabiochiu/t5-small-medium-title-generation'\n",
    "model_name = 'deep-learning-analytics/automatic-title-generation'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#inputs = [f\"Select a suitable label for these keywords: {topic_str}\"]\n",
    "inputs = [f\"Which topic is described by these keywords (response should be between 1 and 12 words): {topic_str}\"]\n",
    "#inputs = text\n",
    "#inputs = topic_str\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=1, max_length=12)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1611e20-995a-471f-9f94-43afabb9907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select the appropriate label for these keywords: field equation star mode theory neutron general state gravity solution'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bc984a3-a989-4dd1-8793-5332bf7caf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Many financial institutions started building conversational AI, prior to the Covid19\n",
    "pandemic, as part of a digital transformation initiative. These initial solutions\n",
    "were high profile, highly personalized virtual assistants — like the Erica chatbot\n",
    "from Bank of America. As the pandemic hit, the need changed as contact centers were\n",
    "under increased pressures. As Cathal McGloin of ServisBOT explains in “how it started,\n",
    "and how it is going,” financial institutions were looking for ways to automate\n",
    "solutions to help get back to “normal” levels of customer service. This resulted\n",
    "in a change from the “future of conversational AI” to a real tactical assistant\n",
    "that can help in customer service. Haritha Dev of Wells Fargo, saw a similar trend.\n",
    "Banks were originally looking to conversational AI as part of digital transformation\n",
    "to keep up with the times. However, with the pandemic, it has been more about\n",
    "customer retention and customer satisfaction. In addition, new use cases came about\n",
    "as a result of Covid-19 that accelerated adoption of conversational AI. As Vinita\n",
    "Kumar of Deloitte points out, banks were dealing with an influx of calls about new\n",
    "concerns, like questions around the Paycheck Protection Program (PPP) loans. This\n",
    "resulted in an increase in volume, without enough agents to assist customers, and\n",
    "tipped the scale to incorporate conversational AI. When choosing initial use cases\n",
    "to support, financial institutions often start with high volume, low complexity\n",
    "tasks. For example, password resets, checking account balances, or checking the\n",
    "status of a transaction, as Vinita points out. From there, the use cases can evolve\n",
    "as the banks get more mature in developing conversational AI, and as the customers\n",
    "become more engaged with the solutions. Cathal indicates another good way for banks\n",
    "to start is looking at use cases that are a pain point, and also do not require a\n",
    "lot of IT support. Some financial institutions may have a multi-year technology\n",
    "roadmap, which can make it harder to get a new service started. A simple chatbot\n",
    "for document collection in an onboarding process can result in high engagement,\n",
    "and a high return on investment. For example, Cathal has a banking customer that\n",
    "implemented a chatbot to capture a driver’s license to be used in the verification\n",
    "process of adding an additional user to an account — it has over 85% engagement\n",
    "with high satisfaction. An interesting use case Haritha discovered involved\n",
    "educating customers on financial matters. People feel more comfortable asking a\n",
    "chatbot what might be considered a “dumb” question, as the chatbot is less judgmental.\n",
    "Users can be more ambiguous with their questions as well, not knowing the right\n",
    "words to use, as chatbot can help narrow things down.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769a909-d4f5-4061-a685-a7e74eb536fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

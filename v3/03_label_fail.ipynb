{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a7ab28c-809c-4d44-99f9-c7b0d07a05e1",
   "metadata": {},
   "source": [
    "# Label and describe using an LLM\n",
    "\n",
    "Download and instantiate an LLM from Huggingface.\n",
    "\n",
    "Load the LDA topic models. \n",
    "\n",
    "Prompt the LLM to generate a label and a description for each topic in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98109988-61ca-45e4-ab5c-fda635f632df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atroncos/anaconda3/envs/arxiv_exp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c8b476-7c5d-418b-8e51-211317544f94",
   "metadata": {},
   "source": [
    "Load the topic models fitted in a previous notebook.\n",
    "\n",
    "* lda_gw: Gravitational Waves topics\n",
    "* lda_cscl: Computation and Language topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fdf5aa-e710-4ae9-8494-e847cedad5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/lda_gw.pickle', 'rb') as handle:\n",
    "    lda_gw = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74281fb0-6242-4641-9731-e938655833b2",
   "metadata": {},
   "source": [
    "Get a list of all topics in the model, each topic described by MAX_WORDS \n",
    "\n",
    "* The result is a list of topics. Each topic is represented by a tuple.\n",
    "* The first element of the tuple is a topic number (int).\n",
    "* The second element of the tuple is a list of tuples,\n",
    "* Each tuple represents the words characterising he topic (string) and its corresponding probability (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b4fbba-e19b-4fef-a032-2279fa47da8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30\n",
    "# list[tuples<int, list[tuple<string, float>]>]\n",
    "topics_gw = lda_gw.show_topics(num_words=MAX_WORDS, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b83873-be16-4ffc-99fa-5e812fa1fd5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_str = ', '.join([topic[0] for topic in topics_gw[2][1]])\n",
    "topic_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf11c3b-2fa3-4e16-ae50-f86d1dfc072d",
   "metadata": {},
   "source": [
    "## Setup a LLM pipeline for labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2680bc96-ca0f-4394-b8a6-f7ab05e69ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.08864252269268036, 'token': 11709, 'token_str': 'parameters', 'sequence': 'parameters : model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time'}, {'score': 0.07518714666366577, 'token': 6210, 'token_str': 'definition', 'sequence': 'definition : model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time'}, {'score': 0.04405448958277702, 'token': 4973, 'token_str': 'examples', 'sequence': 'examples : model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time'}, {'score': 0.026667729020118713, 'token': 2951, 'token_str': 'data', 'sequence': 'data : model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time'}, {'score': 0.01710469089448452, 'token': 24110, 'token_str': 'quan', 'sequence': 'quan : model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_name = 'google-bert/bert-base-uncased'\n",
    "labeller = pipeline('fill-mask', model=model_name)\n",
    "outputs = labeller(f\"[MASK]: {topic_str}\")\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8cce883-3742-49d8-b7e1-4a42a200b464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Label this sequence of words model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time is when when when'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google-bert/bert-base-uncased'\n",
    "labeller = pipeline('text-generation', model=model_name)\n",
    "prompt = f\"Label this sequence of words {topic_str} is\"\n",
    "outputs = labeller(prompt, max_new_tokens=3)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0388aef-ffdd-482a-af95-c7d696097ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "The model 'BertForMaskedLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time, and space. time, particle, mass, frequency,.,,, particle, particle, particle,,, time,...,,,,, nonmo matter, particle, particle,,,,,,,,,'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google-bert/bert-base-uncased'\n",
    "#model_name = 'cnicu/t5-small-booksum'\n",
    "#model_name = 't5-small'\n",
    "labeller = pipeline('summarization', model=model_name)\n",
    "outputs = labeller(topic_str, min_length=1, max_new_tokens=50)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40dc9f1-77e2-4ff6-ba1e-c2fc66201f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Which topic is described by these keywords (response should be between 1 and 12 words): model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time.\\n\\nThe basic idea is that time and space are the functions of their interaction forces. These forces can be called electromagnetic field equations and will usually be seen or described as \"linear\" or \"permeable\" forces. In the latter case, the electromagnetic fields are perpendicular to the two axes of space and time and are often \"hidden\". In order to explain all of this it will take a while to get the data and the correct values. The simplest way to build up a model of'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "labeller = pipeline(\"text-generation\", model=model_name)\n",
    "prompt = f\"Which topic is described by these keywords (response should be between 1 and 12 words): {topic_str}\"\n",
    "outputs = labeller(prompt, max_new_tokens=100, pad_token_id=labeller.tokenizer.eos_token_id)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2b279c6-3237-4208-8e35-f0c9a8c3aa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Please summarize this physics topic \"model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time\" in one word: --------------- - physics: --------------- - particle physics: --------------- - space physics: --------------- - high'}]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"gpt2\"\n",
    "labeller = pipeline(\"text-generation\", model=model_name)\n",
    "prompt = f\"Please summarize this physics topic \\\"{topic_str}\\\" in one word: \"\n",
    "outputs = labeller(prompt, max_new_tokens=20, pad_token_id=labeller.tokenizer.eos_token_id)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acdf9a3e-5949-4de4-aa0a-48e581afe6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original text\n",
      " model, spectrum, energy, dark, background, scale, matter, universe, primordial, ray, cosmic, inflation, neutrino, gamma, observation, transition, cmb, cosmological, phase, burst, high, emission, field, power, signal, early, density, parameter, large, time\n",
      "\n",
      "Summary\n",
      " model, spectrum, energy, dark, background\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "input_ids = tokenizer.encode(\"summarize: \" + topic_str, return_tensors='pt', max_length=512, truncation=True)\n",
    "summary_ids = model.generate(input_ids, max_length=10)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nOriginal text\\n\", topic_str)\n",
    "print(\"\\nSummary\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c801d-446c-4167-af26-52165f889f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\", device='cpu', model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75ea201-85f0-487c-98ac-6e6bdab0946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/atroncos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, spectrum, energy, dark, background, scale\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#model_name = 'fabiochiu/t5-small-medium-title-generation'\n",
    "model_name = 'deep-learning-analytics/automatic-title-generation'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#inputs = [f\"Select a suitable label for these keywords: {topic_str}\"]\n",
    "inputs = [f\"Which topic is described by these keywords (response should be between 1 and 12 words): {topic_str}\"]\n",
    "#inputs = text\n",
    "#inputs = topic_str\n",
    "\n",
    "inputs = tokenizer(inputs, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=1, max_length=12)\n",
    "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
    "predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
    "\n",
    "print(predicted_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1611e20-995a-471f-9f94-43afabb9907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Select the appropriate label for these keywords: field equation star mode theory neutron general state gravity solution'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bc984a3-a989-4dd1-8793-5332bf7caf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Many financial institutions started building conversational AI, prior to the Covid19\n",
    "pandemic, as part of a digital transformation initiative. These initial solutions\n",
    "were high profile, highly personalized virtual assistants — like the Erica chatbot\n",
    "from Bank of America. As the pandemic hit, the need changed as contact centers were\n",
    "under increased pressures. As Cathal McGloin of ServisBOT explains in “how it started,\n",
    "and how it is going,” financial institutions were looking for ways to automate\n",
    "solutions to help get back to “normal” levels of customer service. This resulted\n",
    "in a change from the “future of conversational AI” to a real tactical assistant\n",
    "that can help in customer service. Haritha Dev of Wells Fargo, saw a similar trend.\n",
    "Banks were originally looking to conversational AI as part of digital transformation\n",
    "to keep up with the times. However, with the pandemic, it has been more about\n",
    "customer retention and customer satisfaction. In addition, new use cases came about\n",
    "as a result of Covid-19 that accelerated adoption of conversational AI. As Vinita\n",
    "Kumar of Deloitte points out, banks were dealing with an influx of calls about new\n",
    "concerns, like questions around the Paycheck Protection Program (PPP) loans. This\n",
    "resulted in an increase in volume, without enough agents to assist customers, and\n",
    "tipped the scale to incorporate conversational AI. When choosing initial use cases\n",
    "to support, financial institutions often start with high volume, low complexity\n",
    "tasks. For example, password resets, checking account balances, or checking the\n",
    "status of a transaction, as Vinita points out. From there, the use cases can evolve\n",
    "as the banks get more mature in developing conversational AI, and as the customers\n",
    "become more engaged with the solutions. Cathal indicates another good way for banks\n",
    "to start is looking at use cases that are a pain point, and also do not require a\n",
    "lot of IT support. Some financial institutions may have a multi-year technology\n",
    "roadmap, which can make it harder to get a new service started. A simple chatbot\n",
    "for document collection in an onboarding process can result in high engagement,\n",
    "and a high return on investment. For example, Cathal has a banking customer that\n",
    "implemented a chatbot to capture a driver’s license to be used in the verification\n",
    "process of adding an additional user to an account — it has over 85% engagement\n",
    "with high satisfaction. An interesting use case Haritha discovered involved\n",
    "educating customers on financial matters. People feel more comfortable asking a\n",
    "chatbot what might be considered a “dumb” question, as the chatbot is less judgmental.\n",
    "Users can be more ambiguous with their questions as well, not knowing the right\n",
    "words to use, as chatbot can help narrow things down.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769a909-d4f5-4061-a685-a7e74eb536fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

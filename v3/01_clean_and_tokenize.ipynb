{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "848c4c35-d848-4587-be23-a22f642c4c5a",
   "metadata": {},
   "source": [
    "# Clean and tokenize the data\n",
    "\n",
    "Clean\n",
    "* Load the metadata prepared in `../00_load_metadata.ipynb`\n",
    "* Load abstracts prepared in `../00_load_abstracts.ipynb`\n",
    "* Filter papers by category and keyword:\n",
    "  * arXiv category 'cs.CL' (Computation and Language)\n",
    "  * astro-ph (Astrophysics) and keyword \"Gravitatinal Waves\"\n",
    "  * q-bio.GN (Genomics) and keyword: “CRISPR”\n",
    "  * Merge with metadata dataframe, check that all entries have an abstract\n",
    "  * Filter by keyword\n",
    "\n",
    "Split data \n",
    "* Split data into train/validate/test\n",
    "* Save the clean data on NLP\n",
    "\n",
    "Tokenize\n",
    "* Aplly pre-processing filters\n",
    "* Apply lemmatization\n",
    "* Save tokenized corpus, one for each train/validate/test dataset on NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f4dbed2-647d-436d-b08a-6e07b31674f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kobv/atroncos/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tok\n",
    "import zipfile as zf\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22d8d1b-98cf-4bb0-9ef9-2303d26dff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6df38-9d7f-4dda-ad6c-8c50e5b77dfd",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf993bf3-4103-437f-b62b-a3d62cc4439f",
   "metadata": {},
   "source": [
    "### Load the metadata prepared in `../00_load_metadata.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a8c852-a313-4882-a469-316a9c1cf18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.23 s, sys: 863 ms, total: 9.09 s\n",
      "Wall time: 9.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load the metadata downloaded from arXiv\n",
    "arxiv_metadata = pd.read_csv(os.path.join(DATA_PATH, 'arxiv_metadata.csv.zip'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94185934-64a5-4516-bee6-58abb1a1c5e8",
   "metadata": {},
   "source": [
    "### Load abstracts prepared in `../00_load_abstracts.ipynb`, merge with metadata dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda3d476-44e7-4222-98c8-805878cf9ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 s, sys: 798 ms, total: 11.8 s\n",
      "Wall time: 11.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:2: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load abstracts extracted data in notebook 00_load_abstracts\n",
    "arxiv_abstracts = pd.read_csv(os.path.join(DATA_PATH, 'arxiv_abstracts.csv.zip'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866eabf6-488c-4ece-bb99-63a06b47c35d",
   "metadata": {},
   "source": [
    "### Filter papers by category:\n",
    "  * arXiv category 'cs.CL' (Computation and Language)\n",
    "  * gr-qc (General Relativity and Quantum Cosmology)\n",
    "  * q-bio.GN (Genomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf09b7c-c571-46c7-9cd3-8cb5084e4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_category(df, categories):\n",
    "    \"\"\"Filter a dataframe with arXiv metadata using an arXiv category.\"\"\"\n",
    "    _idx = []\n",
    "    for subject in arxiv_metadata['categories']:\n",
    "        _idx.append(sum([category in subject for category in categories]) > 0)\n",
    "    return(df[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0beb2086-4bdd-4d85-b648-3393707ea8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papers on Computation and Language: original category 'cs.CL'\n",
    "arxiv_cscl = filter_by_category(arxiv_metadata, ['cs.CL'])\n",
    "\n",
    "# Papers on General Relativity and Quantum Cosmology: original category 'astro-ph'\n",
    "arxiv_astro = filter_by_category(arxiv_metadata, ['astro-ph', 'gr-qc', 'hep-ph', 'physics.ins-det'])\n",
    "\n",
    "# Papers on Genomics: original category 'q-bio.GN'\n",
    "arxiv_gen = filter_by_category(arxiv_metadata, ['cs', 'q-bio', 'stat', 'physics.bio-ph'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dc14d-78f1-4d25-a7ee-471c9fc5292c",
   "metadata": {},
   "source": [
    "### Merge with metadata dataframe, check that all entries have an abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1250eba1-791b-49c3-a4b1-1f26c8e65ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_metadata_abstracts(df_metadata, df_abstracts):\n",
    "    # merge metadata and abstracts dataframes, keep row only if both present\n",
    "    df_temp = df_abstracts[df_abstracts.id.isin(df_metadata.id)]\n",
    "    df_merged = pd.merge(df_metadata, df_temp, on='id')\n",
    "    # check that all entries have an abstract\n",
    "    idx = df_merged['abstract'].isna()\n",
    "    df_merged = df_merged[~idx]\n",
    "    return(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28e8797-e8a5-4860-bb44-a2c1e2c34b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge papers on Computation and Language\n",
    "arxiv_cscl_merged = merge_metadata_abstracts(arxiv_cscl, arxiv_abstracts)\n",
    "\n",
    "# merge papers on Astrophysics\n",
    "arxiv_astro_merged = merge_metadata_abstracts(arxiv_astro, arxiv_abstracts)\n",
    "\n",
    "# merge papers on General Relativity and Quantum Cosmology\n",
    "arxiv_gen_merged = merge_metadata_abstracts(arxiv_gen, arxiv_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc3bf6-4534-454f-8674-9cbcd2db793c",
   "metadata": {},
   "source": [
    "### Filter by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c15072-b16a-4b8b-8fcc-4a9dee39d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_keyword(df, keyword):\n",
    "    \"\"\"Filter a dataframe with arXiv metadata and abstract using a keyword search in the abstract.\"\"\"\n",
    "    keyword = keyword.lower()\n",
    "    _idx = [keyword in abstract.lower() for abstract in df['abstract']]\n",
    "    _filtered = df[_idx]\n",
    "    return(_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9ddfbf-b9f9-44a5-a021-d2036b44a30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Papers on gravitational waves\n",
    "arxiv_gw_merged = filter_by_keyword(arxiv_astro_merged, \"gravitational wave\")\n",
    "\n",
    "# Papers on CRISPR\n",
    "arxiv_crispr_merged = filter_by_keyword(arxiv_gen_merged, \"CRISPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78628d4b-547f-4d3a-9064-5c71e7c309a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54567 research papers on 'Computer and Language' in the dataset.\n",
      "There are 14779 research papers on 'Gravitational Waves' in the dataset.\n",
      "There are 53 research papers on 'CRISPR' in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(arxiv_cscl_merged)} research papers on 'Computer and Language' in the dataset.\")\n",
    "print(f\"There are {len(arxiv_gw_merged)} research papers on 'Gravitational Waves' in the dataset.\")\n",
    "print(f\"There are {len(arxiv_crispr_merged)} research papers on 'CRISPR' in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a238f4-589d-4ce5-8ea1-f1257897824e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fee0d033-2a4b-426d-9ae3-312516f2a951",
   "metadata": {},
   "source": [
    "## Split the data into train / validate / test datasets\n",
    "\n",
    "\"train\"\n",
    "> A percent of the texts reserved for fitting the model.\n",
    "\n",
    "\"validate\"\n",
    "> A percent of the texts reserved for computing perplexity when fitting the model's k-parameter, and searching for best parameters.\n",
    "\n",
    "\"test\"\n",
    "> A percent of the texts reserved for testing hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded9e9b8-fc72-477b-b5e0-a5f15ee600f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    train, test = train_test_split(df, test_size=0.5)\n",
    "    validate, test = train_test_split(test, test_size=0.5)\n",
    "    return(train, validate, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d8e28b5-f3de-47c9-ad7f-f9948663f0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset for cs.CL has 27283 rows, the validate dataset 13642 rows, the test dataset 13642 rows\n"
     ]
    }
   ],
   "source": [
    "# split papers on Computation and Language\n",
    "train_cscl, validate_cscl, test_cscl = split(arxiv_cscl_merged)\n",
    "print(f\"The train dataset for cs.CL has {train_cscl.shape[0]} rows, the validate dataset {validate_cscl.shape[0]} rows, the test dataset {test_cscl.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0914c35e-67a4-4d07-9cb8-fe38b44bf487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset for gw has 7389 rows, the validate dataset 3695 rows, the test dataset 3695 rows\n"
     ]
    }
   ],
   "source": [
    "# split papers on Gravitational Waves\n",
    "train_gw, validate_gw, test_gw = split(arxiv_gw_merged)\n",
    "print(f\"The train dataset for gw has {train_gw.shape[0]} rows, the validate dataset {validate_gw.shape[0]} rows, the test dataset {test_gw.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e023b-f356-4f42-afc6-fb59d26537a9",
   "metadata": {},
   "source": [
    "### Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55162e94-e1f1-4d16-a6c1-28b99d1c2e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(filename, path, df):\n",
    "    with zf.ZipFile(path, 'w') as ziparchive:\n",
    "        ziparchive.writestr(filename, df.to_csv())\n",
    "\n",
    "def save_datasets(name, train, validate, test):\n",
    "    filename_train = f\"{name}_train.csv\"\n",
    "    path_train = os.path.join(DATA_PATH, f\"{filename_train}.zip\")\n",
    "    save_dataset(filename_train, path_train, train)\n",
    "    \n",
    "    filename_validate = f\"{name}_validate.csv\"\n",
    "    path_validate = os.path.join(DATA_PATH, f\"{filename_validate}.zip\")\n",
    "    save_dataset(filename_validate, path_validate, validate)\n",
    "\n",
    "    filename_test = f\"{name}_test.csv\"\n",
    "    path_test = os.path.join(DATA_PATH, f\"{filename_test}.zip\")\n",
    "    save_dataset(filename_test, path_test, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e63e271-87c4-41a5-b8d9-b0ae3f07b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned data on Computation and Language\n",
    "save_datasets('cscl', train_cscl, validate_cscl, test_cscl)\n",
    "\n",
    "# save the cleaned data on Gravitational Waves\n",
    "save_datasets('gw', train_gw, validate_gw, test_gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a7fb7-9e2c-48e0-8c9a-34ce426416f2",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e81b9a-f917-41a0-9e22-678189217298",
   "metadata": {},
   "source": [
    "Apply pre-processing filters: strip_tags, strip_punctuation, strip_multiple_whitespaces, stric_numeric, remove_stopwords; strip_short\n",
    "\n",
    "Apply lemmatization to the list of words.\n",
    "\n",
    "The tokenize functions are in the `tok.py` package provided in the same directory as the notebooks.\n",
    "\n",
    "see: https://github.com/piskvorky/gensim/blob/develop/gensim/parsing/preprocessing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def8a678-f5e2-434e-b7ec-b65a10c1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary with all the words in the complete CSCL dataset\n",
    "texts_cscl = tok.clean(arxiv_cscl_merged['abstract'])\n",
    "dictionary_cscl = tok.make_dictionary(texts_cscl)\n",
    "\n",
    "# make a dictionary with all the words in the complete GW dataset\n",
    "texts_gw = tok.clean(arxiv_gw_merged['abstract'])\n",
    "dictionary_gw = tok.make_dictionary(texts_gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff5b6bd3-1b61-40d3-b838-20bcece2cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dictionary, df):\n",
    "    _texts = tok.clean(df['abstract'])\n",
    "    return(tok.make_corpus(dictionary, _texts))\n",
    "\n",
    "def tokenize_datasets(dictionary, train, validate, test):\n",
    "    corpus_train = tokenize_dataset(dictionary, train)\n",
    "    corpus_validate = tokenize_dataset(dictionary, validate)\n",
    "    corpus_test = tokenize_dataset(dictionary, test)\n",
    "    return(corpus_train, corpus_validate, corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50da44d9-f9d5-4758-a803-ea61c5d04454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data on Computation and Language\n",
    "corpus_train_cscl, corpus_validate_cscl, corpus_test_cscl = tokenize_datasets(dictionary_cscl, train_cscl, validate_cscl, test_cscl)\n",
    "\n",
    "# Tokenize data on Gravitational Waves\n",
    "corpus_train_gw, corpus_validate_gw, corpus_test_gw = tokenize_datasets(dictionary_gw, train_gw, validate_gw, test_gw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a149c-83a2-42cd-8292-a62c2472ccf2",
   "metadata": {},
   "source": [
    "### Save tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d181f2f-db89-4bbc-acd9-77594c66a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "with open(os.path.join(DATA_PATH, 'dictionary_cscl.pickle'), 'wb') as handle:\n",
    "    pickle.dump(dictionary_cscl, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'dictionary_gw.pickle'), 'wb') as handle:\n",
    "    pickle.dump(dictionary_gw, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3603c7d-83b7-4405-923f-1695f051fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenized_dataset(path, obj):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_tokenized_datasets(name, train, validate, test):\n",
    "    path_train = os.path.join(DATA_PATH, f\"corpus_train_{name}.pickle\")\n",
    "    save_tokenized_dataset(path_train, train)\n",
    "\n",
    "    path_validate = os.path.join(DATA_PATH, f\"corpus_validate_{name}.pickle\")\n",
    "    save_tokenized_dataset(path_validate, validate)\n",
    "\n",
    "    path_test = os.path.join(DATA_PATH, f\"corpus_test_{name}.pickle\")\n",
    "    save_tokenized_dataset(path_test, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c75e678-5fe1-4b73-8153-4332339dfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data on Computation and Language\n",
    "save_tokenized_datasets('cscl', corpus_train_cscl, corpus_validate_cscl, corpus_test_cscl)\n",
    "\n",
    "# save data on Gravitational Waves\n",
    "save_tokenized_datasets('gw', corpus_train_gw, corpus_validate_gw, corpus_test_gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d0ae2-d7e3-4ec1-89ab-588a326a4685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

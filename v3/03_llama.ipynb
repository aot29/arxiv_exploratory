{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32a6019-86d2-4516-9750-2ded9339c248",
   "metadata": {},
   "source": [
    "# Label and describe using an LLM\n",
    "\n",
    "Download and instantiate an LLM from Huggingface.\n",
    "\n",
    "Load the LDA topic models. \n",
    "\n",
    "Prompt the LLM to generate a label and a description for each topic in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624a770b-cedd-4e31-887b-acc692a3b988",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/kobv/atroncos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253401c-3595-40eb-9968-9a5d6628a8b9",
   "metadata": {},
   "source": [
    "Load the topic models fitted in a previous notebook.\n",
    "\n",
    "* lda_gw: Gravitational Waves topics\n",
    "* lda_cscl: Computation and Language topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542421d1-a7ff-4cf6-a4e1-9b9cff4ecf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA gravitational waves model\n",
    "with open('../models/lda_gw.pickle', 'rb') as handle:\n",
    "    lda_gw = pickle.load(handle)\n",
    "\n",
    "# Ensemble LDA gravitational waves model\n",
    "with open('../models/ensemble_gw.pickle', 'rb') as handle:\n",
    "    ensemble_gw = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bcb16c7-9f47-4f01-a50b-6377b500a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA computing & language model\n",
    "with open('../models/lda_cscl.pickle', 'rb') as handle:\n",
    "    lda_cscl = pickle.load(handle)\n",
    "\n",
    "# Ensemble LDA computing & language model\n",
    "with open('../models/ensemble_cscl.pickle', 'rb') as handle:\n",
    "    ensemble_cscl = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb64b0ca-dbb5-41c0-9381-3a163f4341e6",
   "metadata": {},
   "source": [
    "Get a list of all topics in the model, each topic described by MAX_WORDS \n",
    "\n",
    "* The result is a list of topics. Each topic is represented by a tuple.\n",
    "* The first element of the tuple is a topic number (int).\n",
    "* The second element of the tuple is a list of tuples,\n",
    "* Each tuple represents the words characterising he topic (string) and its corresponding probability (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c85b7775-3cd3-4610-982c-4f1ffa5be5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 30\n",
    "\n",
    "# The expected format for the topics list is:\n",
    "# list[tuples<int, list[tuple<string, float>]>]\n",
    "\n",
    "# a Gensim LDA model\n",
    "topics_gw = lda_gw.show_topics(num_words=MAX_WORDS, formatted=False)\n",
    "\n",
    "# an Ensemble lDA model, has to be converted to Gensim LDA first\n",
    "topics_ensemble_gw = ensemble_gw.generate_gensim_representation().show_topics(num_topics=-1, num_words=MAX_WORDS, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0ef802a-40b7-463a-bae7-ae3b3cbba172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Gensim LDA model\n",
    "topics_cscl = lda_cscl.show_topics(num_words=MAX_WORDS, formatted=False)\n",
    "\n",
    "# an Ensemble lDA model, has to be converted to Gensim LDA first\n",
    "topics_ensemble_cscl = ensemble_cscl.generate_gensim_representation().show_topics(num_topics=-1, num_words=MAX_WORDS, formatted=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edbcd20-4cb4-491c-bfe0-5eb1ca4ad227",
   "metadata": {},
   "source": [
    "### Using Llama gated model\n",
    "\n",
    "1. Go to huggingface, login, go to `settings/access tokens` \n",
    "2. Create a new READ token, save it to ../token.txt\n",
    "3. Go here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8Band accept the usage conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a255ab19-9ca0-46e8-ac85-127f1eddec24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/kobv/atroncos/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "with open('../token.txt', 'r') as handle:\n",
    "    token = handle.read()\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95683a43-5540-4701-9dd2-b2ea7650dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_str(topic, max=None):\n",
    "    \"\"\"Return the terms describing a topic as a string\n",
    "    topic: list of tuples<string, float>\n",
    "    \"\"\"\n",
    "    if not max:\n",
    "        resp = ', '.join([term[0] for term in topic[1]])\n",
    "    else:\n",
    "        resp = ', '.join([term[0] for term in topic[1][:max]])    \n",
    "    return(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5732c-490d-4522-9352-a306f994e63c",
   "metadata": {},
   "source": [
    "An wrapper class for the Llama LLM: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7c22d7f-ab42-4f45-a7b6-c23fa717664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Promptable(ABC):\n",
    "    @abstractmethod\n",
    "    def predict_label(self, category, terms): pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_description(self, category, terms): pass\n",
    "\n",
    "class Llama(Promptable):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "    def predict_label(self, category, terms):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are an AI assistant specialised in {category}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"What short, concise and human-readable label best describes the topic characterised by these terms: {terms}? Output only the label\"},\n",
    "        ]\n",
    "        outputs = self.pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=25,\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"][-1]['content']\n",
    "    \n",
    "    def predict_description(self, category, terms):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": f\"You are an AI assistant specialised in {category}\"},\n",
    "            {\"role\": \"user\", \"content\": f\"What best describes the topic characterised by these terms: {terms}?\"},\n",
    "        ]\n",
    "        outputs = self.pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"][-1]['content']\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        del self.pipeline\n",
    "        # release GPU memory\n",
    "        gc.collect()  # explicitly call garbage collector\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43548260-9101-4a32-9b44-802f17cb5ceb",
   "metadata": {},
   "source": [
    "A function to create labels for all topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "525b21e7-be0e-4ffe-b2b9-ef437ea33d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topic_labels(category, topics, model):\n",
    "    \"\"\"\n",
    "    Predict label for a list of topics.\n",
    "\n",
    "    category: an arxiv category, see https://arxiv.org/category_taxonomy, e.g. \"General Relativity and Quantum Cosmology\"\n",
    "    topics: topics in an LDA model, obtained through lda_XX.show_topics(num_words=MAX_WORDS, formatted=False)\n",
    "    model: Object implementing abstract class \"Promptable\" (see above)\n",
    "    returns: dataFrame with columns: topic id, label\n",
    "    \"\"\"\n",
    "    topic_labels = []  # topic label, string, generated by LLM\n",
    "    topic_ids = []  # topic id, numeric\n",
    "    topic_main_words = []  # the first 5 keywords, as string\n",
    "    topic_descriptions = []  # description of a topic\n",
    "    \n",
    "    topics_range = [topic[0] for topic in topics]\n",
    "    for count, topic in enumerate(topics):\n",
    "        print(f\"Processing topic {count} / {len(topics_range)}\")\n",
    "        topic_id = topic[0]\n",
    "        terms = get_topic_str(topic) # all keywords, as string\n",
    "\n",
    "        # label\n",
    "        label = model.predict_label(category, terms)\n",
    "        topic_labels.append(label)\n",
    "\n",
    "        # numeric topic id\n",
    "        topic_ids.append(topic_id)\n",
    "\n",
    "        # topic keywords\n",
    "        topic_main_words.append(get_topic_str(topic, 5))\n",
    "\n",
    "        # description\n",
    "#        prompt = f\"Describe the topic in the \\\"{category}\\\" category characterised by these terms: {terms}.\"\n",
    "#        description = model.predict_description(category, terms)\n",
    "#        topic_descriptions.append(description)\n",
    "\n",
    "    return(pd.DataFrame.from_dict({'Topic': topic_ids, 'First 5 keywords': topic_main_words, 'Label': topic_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7b5d4-9506-448d-80b1-117f08329741",
   "metadata": {},
   "source": [
    "### Topics for Gravitational Waves LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f884b8c-41a3-4f4f-aa78-d669192e0279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.81it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 0 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 2 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 3 / 4\n",
      "CPU times: user 59.8 s, sys: 1.56 s, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Llama() as model:\n",
    "    topics_gw_df = predict_topic_labels(\"General Relativity and Quantum Cosmology\", topics_gw, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045f7278-6ffc-4ddb-9185-d30f833b02bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>First 5 keywords</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>detector, signal, data, noise, frequency</td>\n",
       "      <td>Gravitational Wave Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>binary, hole, mass, black, star</td>\n",
       "      <td>Gravitational Wave Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>model, spectrum, energy, dark, background</td>\n",
       "      <td>Cosmic Microwave Background (CMB) Observations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>field, theory, mode, gravity, equation</td>\n",
       "      <td>General Relativity and Quantum Cosmology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                           First 5 keywords  \\\n",
       "0      0   detector, signal, data, noise, frequency   \n",
       "1      1            binary, hole, mass, black, star   \n",
       "2      2  model, spectrum, energy, dark, background   \n",
       "3      3     field, theory, mode, gravity, equation   \n",
       "\n",
       "                                            Label  \n",
       "0                    Gravitational Wave Astronomy  \n",
       "1                    Gravitational Wave Astronomy  \n",
       "2  Cosmic Microwave Background (CMB) Observations  \n",
       "3        General Relativity and Quantum Cosmology  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_gw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc564cd-e119-405f-83de-4792126faf0d",
   "metadata": {},
   "source": [
    "### Topics for Gravitational Waves ensemble LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96de38d0-24b6-4803-9a51-39cffead22d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.87it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 0 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 1 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 2 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 3 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 4 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 5 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 6 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 7 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 8 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 9 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 11 / 12\n",
      "CPU times: user 2min 56s, sys: 2.7 s, total: 2min 59s\n",
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Llama() as model:\n",
    "    topics_ensemble_gw = predict_topic_labels(\"General Relativity and Quantum Cosmology\", topics_ensemble_gw, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "835d7149-8b8d-45b6-8a64-98fcc3cff298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>First 5 keywords</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>hole, black, binary, mass, spin</td>\n",
       "      <td>Gravitational Wave Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>search, signal, detector, data, ligo</td>\n",
       "      <td>Gravitational Wave Astronomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pulsar, timing, array, noise, data</td>\n",
       "      <td>Millisecond Pulsar Timing Array (MPTA) Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>star, neutron, merger, mass, binary</td>\n",
       "      <td>Binary Neutron Star Merger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>mode, star, instability, frequency, neutron</td>\n",
       "      <td>Rotating Neutron Star Oscillations (RNSEO)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>ray, gamma, burst, energy, emission</td>\n",
       "      <td>Gamma-Ray Burst (GRB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>binary, source, distance, parameter, mass</td>\n",
       "      <td>Binary Black Hole Merger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>theory, gravity, general, scalar, field</td>\n",
       "      <td>General Relativity and Beyond</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>model, dark, spectrum, inflation, matter</td>\n",
       "      <td>Cosmological Inflation Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>transition, phase, model, order, electroweak</td>\n",
       "      <td>Electroweak Baryogenesis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>noise, detector, interferometer, frequency, sensitivity</td>\n",
       "      <td>Gravitational Wave Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>string, cosmic, model, energy, loop</td>\n",
       "      <td>\"Cosmological String Theory\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic                                         First 5 keywords  \\\n",
       "0       0                          hole, black, binary, mass, spin   \n",
       "1       1                     search, signal, detector, data, ligo   \n",
       "2       2                       pulsar, timing, array, noise, data   \n",
       "3       3                      star, neutron, merger, mass, binary   \n",
       "4       4              mode, star, instability, frequency, neutron   \n",
       "5       5                      ray, gamma, burst, energy, emission   \n",
       "6       6                binary, source, distance, parameter, mass   \n",
       "7       7                  theory, gravity, general, scalar, field   \n",
       "8       8                 model, dark, spectrum, inflation, matter   \n",
       "9       9             transition, phase, model, order, electroweak   \n",
       "10     10  noise, detector, interferometer, frequency, sensitivity   \n",
       "11     11                      string, cosmic, model, energy, loop   \n",
       "\n",
       "                                              Label  \n",
       "0                      Gravitational Wave Astronomy  \n",
       "1                      Gravitational Wave Astronomy  \n",
       "2   Millisecond Pulsar Timing Array (MPTA) Analysis  \n",
       "3                        Binary Neutron Star Merger  \n",
       "4        Rotating Neutron Star Oscillations (RNSEO)  \n",
       "5                             Gamma-Ray Burst (GRB)  \n",
       "6                          Binary Black Hole Merger  \n",
       "7                     General Relativity and Beyond  \n",
       "8                      Cosmological Inflation Model  \n",
       "9                          Electroweak Baryogenesis  \n",
       "10                     Gravitational Wave Detection  \n",
       "11                     \"Cosmological String Theory\"  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_ensemble_gw.sort_values(by='Topic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6fa28-7972-4de2-b603-61bf04fba542",
   "metadata": {},
   "source": [
    "### Topics Computing & Language LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb66d4e4-b4c4-4818-86be-c2b8b57c8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.88it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 0 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 1 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 2 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 3 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 4 / 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 5 / 6\n",
      "CPU times: user 1min 10s, sys: 1.76 s, total: 1min 12s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Llama() as model:\n",
    "    topics_cscl_df = predict_topic_labels(\"Computation and Language\", topics_cscl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d9efad7-57e0-4050-bf26-a59034ec899f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>First 5 keywords</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>data, research, user, analysis, text</td>\n",
       "      <td>Social Media Sentiment Analysis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>task, data, training, performance, learning</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>question, llm, human, knowledge, task</td>\n",
       "      <td>Conversational AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>translation, speech, english, data, machine</td>\n",
       "      <td>Machine Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>word, based, method, sentence, representation</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>image, text, speech, visual, feature</td>\n",
       "      <td>Multimodal Machine Learning</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                               First 5 keywords  \\\n",
       "0      0           data, research, user, analysis, text   \n",
       "1      1    task, data, training, performance, learning   \n",
       "2      2          question, llm, human, knowledge, task   \n",
       "3      3    translation, speech, english, data, machine   \n",
       "4      4  word, based, method, sentence, representation   \n",
       "5      5           image, text, speech, visual, feature   \n",
       "\n",
       "                               Label  \n",
       "0    Social Media Sentiment Analysis  \n",
       "1  Natural Language Processing (NLP)  \n",
       "2                  Conversational AI  \n",
       "3                Machine Translation  \n",
       "4  Natural Language Processing (NLP)  \n",
       "5        Multimodal Machine Learning  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_cscl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9181a-a0a0-423f-8d13-a1044c84b3e6",
   "metadata": {},
   "source": [
    "### Topics Computing & Language ensemble LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e21c21-0c9a-4189-adeb-bca686619733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.91it/s]\n",
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 0 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 1 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 2 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 3 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 4 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 5 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 6 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 7 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 8 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 9 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 10 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 11 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 12 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 13 / 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic 14 / 15\n",
      "CPU times: user 2min 39s, sys: 2.52 s, total: 2min 42s\n",
      "Wall time: 2min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with Llama() as model:\n",
    "    topics_ensemble_cscl = predict_topic_labels(\"Computation and Language\", topics_ensemble_cscl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df53d667-aff8-4f3f-8ce0-f5f6eae7d9ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>First 5 keywords</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>translation, machine, data, nmt, neural</td>\n",
       "      <td>Neural Machine Translation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>question, answer, answering, task, reasoning</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>llm, task, large, performance, prompt</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>speech, data, task, recognition, training</td>\n",
       "      <td>Speech Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>bias, gender, data, task, based</td>\n",
       "      <td>Debiasing NLP Models</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dialogue, task, state, system, human</td>\n",
       "      <td>Dialogue Systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>style, knowledge, task, transfer, text</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>evaluation, human, metric, task, summarization</td>\n",
       "      <td>Text Summarization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>topic, approach, document, method, word</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>event, argument, extraction, task, method</td>\n",
       "      <td>Information Extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>code, task, generation, training, dataset</td>\n",
       "      <td>Large Language Model (LLM) Development</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>grammar, parsing, based, syntactic, parser</td>\n",
       "      <td>Natural Language Processing (NLP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>graph, method, word, approach, network</td>\n",
       "      <td>Natural Language Processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>news, social, medium, information, data</td>\n",
       "      <td>Fake News Detection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>speaker, speech, based, method, task</td>\n",
       "      <td>Speech Synthesis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic                                First 5 keywords  \\\n",
       "0       0         translation, machine, data, nmt, neural   \n",
       "1       1    question, answer, answering, task, reasoning   \n",
       "2       2           llm, task, large, performance, prompt   \n",
       "3       3       speech, data, task, recognition, training   \n",
       "4       4                 bias, gender, data, task, based   \n",
       "5       5            dialogue, task, state, system, human   \n",
       "6       6          style, knowledge, task, transfer, text   \n",
       "7       7  evaluation, human, metric, task, summarization   \n",
       "8       8         topic, approach, document, method, word   \n",
       "9       9       event, argument, extraction, task, method   \n",
       "10     10       code, task, generation, training, dataset   \n",
       "11     11      grammar, parsing, based, syntactic, parser   \n",
       "12     12          graph, method, word, approach, network   \n",
       "13     13         news, social, medium, information, data   \n",
       "14     14            speaker, speech, based, method, task   \n",
       "\n",
       "                                     Label  \n",
       "0               Neural Machine Translation  \n",
       "1        Natural Language Processing (NLP)  \n",
       "2        Natural Language Processing (NLP)  \n",
       "3                       Speech Recognition  \n",
       "4                     Debiasing NLP Models  \n",
       "5                         Dialogue Systems  \n",
       "6              Natural Language Processing  \n",
       "7                       Text Summarization  \n",
       "8              Natural Language Processing  \n",
       "9                   Information Extraction  \n",
       "10  Large Language Model (LLM) Development  \n",
       "11       Natural Language Processing (NLP)  \n",
       "12             Natural Language Processing  \n",
       "13                     Fake News Detection  \n",
       "14                        Speech Synthesis  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_ensemble_cscl.sort_values(by='Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1698864-2c8e-46ec-bbc7-9bbe8fbdb1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
